{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import etree\n",
    "from fastcore.xtras import Path\n",
    "from urllib.parse import urljoin, unquote\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import os\n",
    "from fastcore.parallel import parallel\n",
    "import copy\n",
    "\n",
    "def get_all_a_tags(page_url: str, html: etree._Element):\n",
    "    \"\"\"\n",
    "    Retrieves all <a> tags from a given webpage URL using XPath,\n",
    "    resolving both absolute and relative URLs, and handling encoded URLs\n",
    "    and non-English anchor text.\n",
    "    \n",
    "    Args:\n",
    "    - page_url (str): The URL of the page to fetch and parse.\n",
    "    - html (etree._Element): The lxml element representing the HTML content.\n",
    "\n",
    "    Returns:\n",
    "    List of tuples (decoded_full_url, anchor_text), where:\n",
    "    - decoded_full_url (str): The decoded full URL (resolved for relative links, if valid).\n",
    "    - anchor_text (str): The text inside the anchor tag (in any language).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check for <base> tag and get base URL\n",
    "        base_url = get_base_url(html, page_url)\n",
    "        \n",
    "        # Use XPath to find all <a> tags\n",
    "        a_tags = html.xpath('//a')\n",
    "\n",
    "        # List to store (full_url, anchor_text)\n",
    "        result = []\n",
    "\n",
    "        # Iterate over all <a> tags\n",
    "        for a_tag in a_tags:\n",
    "            href = a_tag.get('href')\n",
    "\n",
    "            # Skip invalid hrefs (empty, JavaScript, mailto, tel, fragments)\n",
    "            if href and not href.startswith(('javascript:', 'mailto:', 'tel:', 'ftp:', '#')):\n",
    "                # Resolve the full URL using urljoin (to handle relative URLs)\n",
    "                full_url = urljoin(base_url, href)\n",
    "\n",
    "                # Decode URL if it is encoded (e.g., %20 for space, non-ASCII characters)\n",
    "                decoded_full_url = unquote(full_url)\n",
    "\n",
    "                # Get the anchor text using ''.join() to concatenate text and tail\n",
    "                anchor_text = ''.join(a_tag.itertext()).strip()\n",
    "\n",
    "                # Append (decoded_full_url, anchor_text) to result\n",
    "                result.append((decoded_full_url, anchor_text))\n",
    "\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:  # Changed to Exception for broader error handling\n",
    "        print(f\"Error processing the page: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_base_url(html: etree._Element, page_url: str):\n",
    "    \"\"\"\n",
    "    Extracts the base URL from the <base> tag if present, or falls back to the page URL.\n",
    "    \n",
    "    Args:\n",
    "    - html (etree._Element): The lxml element representing the HTML content.\n",
    "    - page_url (str): The original page URL.\n",
    "    \n",
    "    Returns:\n",
    "    str: The base URL to resolve relative links.\n",
    "    \"\"\"\n",
    "    base_tag = html.xpath('//base')\n",
    "    if base_tag and base_tag[0].get('href'):\n",
    "        return base_tag[0].get('href')\n",
    "    return page_url  # Fallback to the page URL if no <base> tag is found\n",
    "\n",
    "\n",
    "import logging\n",
    "def get_index_page_extract_link(url):\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    html = etree.HTML(str(soup))\n",
    "\n",
    "    content = copy.deepcopy(html.xpath('//figure[@class=\"post-thumbnail\"]')) # the article html\n",
    "    assert len(content) > 0, \"No content found\" # as each page contains few news articles\n",
    "\n",
    "    posts_li = [j for i in content for j in get_all_a_tags(url,i)]\n",
    "    \n",
    "    return posts_li\n",
    "  except Exception as e:\n",
    "    logging.exception(\"An error occurred: %s\", e)\n",
    "    return []\n",
    "\n",
    "lis = [\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/state-news/',\n",
    "    'page_limit': 4768,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/national-news/',\n",
    "    'page_limit': 4768,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/metro-news/',\n",
    "    'page_limit': 2297,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/international-news/',\n",
    "    'page_limit': 1627,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/sports-news/',\n",
    "    'page_limit': 2115,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/business/',\n",
    "    'page_limit': 896,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/editorial/',\n",
    "    'page_limit': 649,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/entertainment/',\n",
    "    'page_limit': 1253,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/district-news/',\n",
    "    'page_limit': 5903,\n",
    "  },\n",
    "  {\n",
    "    'url': 'https://www.dharitri.com/category/fursat/',\n",
    "    'page_limit': 1082,\n",
    "  },\n",
    "\n",
    "]\n",
    "import re\n",
    "get_fname = lambda url : re.findall(r'/category/(.+)/$', url)[0] + '.csv'\n",
    "#print(get_fname('https://www.dharitri.com/category/state-news/'))\n",
    "final_lis = [{**i, 'fname': get_fname(i['url'])} for i in lis]\n",
    "#\n",
    "# final_lis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.dharitri.com/category/district-news/',\n",
       "  'page_limit': 5903},\n",
       " {'url': 'https://www.dharitri.com/category/fursat/', 'page_limit': 1082}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dharitri.com/category/district-news/ 5903 district-news.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dharitri.com/category/fursat/ 1082 fursat.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_new = lambda base_url, i : f'{base_url}/page/{i}/'\n",
    "for i in final_lis[-2:]:\n",
    "        url, page_limit, fname = i.values()\n",
    "        print(url, page_limit, fname)\n",
    "\n",
    "        \n",
    "        # no of pages 1 : page_limit\n",
    "        def process_page(i):\n",
    "            idx = state_new(url, i)\n",
    "            return get_index_page_extract_link(idx)\n",
    "\n",
    "        content_links = parallel(process_page, range(1, page_limit), n_workers=12, progress=True)\n",
    "        \n",
    "        article_links = set([ j[0] for i in content_links for j in i]  )\n",
    "        \n",
    "        df = pd.DataFrame(list(article_links), columns=['article_link'])\n",
    "\n",
    "        df.to_csv(fname, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      10\n"
     ]
    }
   ],
   "source": [
    "!ls | grep \".csv\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
